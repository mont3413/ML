{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c3c23c-3b30-4676-8110-974ef4bb58d0",
   "metadata": {},
   "source": [
    "## Сентиментальный анализ отзывов к фильмам с помощью RNN.\n",
    "Загрузим тренировочный и тестовый датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4353d91-509f-4714-8cab-282198f56eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee98385-9cb2-4329-b114-b58b20d2930d",
   "metadata": {},
   "source": [
    "Пример отрицательного отзыва."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdcce0cd-8c9f-4c59-b7ab-035f8b9d628a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?<br /><br />\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[5][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274f162-5bbb-45df-ad9f-b1ccf25516a2",
   "metadata": {},
   "source": [
    "Пример положительного отзыва."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce1707a-f67a-4a63-b5d7-213e0e946468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This film and it's sequel Barry Mckenzie holds his own, are the two greatest comedies to ever be produced. A great story a young Aussie bloke travels to england to claim his inheritance and meets up with his mates, who are just as loveable and innocent as he is.<br /><br />It's chock a block full of great, sayings , where else could you find someone who needs a drink so bad that he's as dry as a dead dingoes donger? great characters, top acting, and it's got great sheilas and more Fosters consumption then any other three films put together. Top notch.<br /><br />And some of the funniest songs you'll ever hear, and it's full of great celebrities. Definitely my two favourite films of all time, I watch them at least once a fortnight.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset)[-3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369d8a4-92c7-48e8-8fec-df85e0d37361",
   "metadata": {},
   "source": [
    "Разделим на тренировочный и валидационный датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "038e8836-49e7-415c-9ad6-9543d3e09ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dataset, valid_dataset = random_split(list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c55a4e-20d4-49b2-b18b-57ea0a78d3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 106983\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Удаление всех символов кроме букв, цифр и пробелов\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Удаление лишних пробелов\n",
    "    tokenized = text.split()     \n",
    "    return tokenized\n",
    "\n",
    "token_counts = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print(f'Num tokens: {len(token_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7923827-3b39-4c49-9ef5-3f93be46687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "sorted_by_freq_tuples = sorted(\n",
    "    token_counts.items(), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e8228bf-31d9-420a-b3ac-a148051010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1.0 if x==2 else 0.0\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9120a438-9ee3-467a-a521-b62ce1648353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size, \n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size, \n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size, \n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f6486f-30dc-49c2-b999-c8bbadd21d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d66a70-b495-4237-bde5-1e09487b16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "torch.manual_seed(0)\n",
    "model = RNN(vocab_size, embed_dim, \n",
    "            rnn_hidden_size, fc_hidden_size)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e8662ef-21b7-48ac-9115-21042395ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    train_acc = 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_acc += (\n",
    "            (pred >= 0.5).float() == label_batch\n",
    "        ).float().sum().item()\n",
    "    return train_acc / len(list(dataloader.dataset))\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    eval_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            eval_acc += (\n",
    "                (pred >= 0.5).float() == label_batch\n",
    "            ).float().sum().item()\n",
    "        \n",
    "    return eval_acc / len(list(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f215da82-43cc-4cae-9a5a-e55c018d99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b75ab89efea80010\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b75ab89efea80010\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='./logs'\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc = train(train_dl)\n",
    "    valid_acc = evaluate(valid_dl)\n",
    "    writer.add_scalars('Train/Valid Accuracy',\n",
    "                      {\n",
    "                          'Train Accuracy': train_acc,\n",
    "                          'Valid Accuracy': valid_acc\n",
    "                      }, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "612911c8-c40a-4eec-a31d-b140b918e5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8631\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(test_dl)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b3928ea-d55a-4a23-a3b8-eaea931a1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'sentiment_analysis_RNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2278e-924e-41d8-9a3c-d23e8ee7ca43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
