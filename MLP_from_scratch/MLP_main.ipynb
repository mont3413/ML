{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54ea903-e377-490e-bb7f-d0e83fc5b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caca5629-4955-4398-b99a-536ab7e87cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -10, 10)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    x = np.clip(x, -10, 10)\n",
    "    return 2 / (1 + np.exp(-x)) - 1\n",
    "\n",
    "def relu_grad_activation(x):\n",
    "    return x > 0\n",
    "\n",
    "def leaky_relu_grad_activation(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "    \n",
    "def sigmoid_grad_activation(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_grad_activation(x):\n",
    "    return 1 - x**2\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.array(x)\n",
    "    x -= np.max(x, axis=1, keepdims=True)  # Stabilize\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def identity_activation(x):\n",
    "    return x\n",
    "\n",
    "def xavier_initialize(shape):\n",
    "    return np.random.randn(*shape) * np.sqrt(2. / (shape[0] + shape[1]))\n",
    "\n",
    "def he_initialize(shape):\n",
    "    return np.random.randn(*shape) * np.sqrt(2. / shape[0])\n",
    "\n",
    "def clip_gradients(grad, max_value=5.0):\n",
    "    return np.clip(grad, -max_value, max_value)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return np.mean(y_hat.reshape(1, -1) == y.reshape(1, -1))\n",
    "\n",
    "def one_hot_encode_classes(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36dc450-983d-4dfb-aae3-83952073fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer():\n",
    "    def __init__(self, x=None):\n",
    "        self.values = x\n",
    "        self.n_samples, self.width = None, None\n",
    "        \n",
    "    def forward(self, training=True):\n",
    "        if self.n_samples is None:\n",
    "            self.n_samples, self.width = self.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91e3b4d-d310-4b6a-a213-eec78623651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization():\n",
    "    def __init__(self, width, batch_momentum=0.9):\n",
    "        self.gamma = np.ones((1, width))\n",
    "        self.beta = np.zeros((1, width))\n",
    "        self.running_mean = np.zeros((1, width))\n",
    "        self.running_var = np.ones((1, width))\n",
    "        self.batch_momentum = batch_momentum \n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            # Compute the mean and variance for the current batch\n",
    "            self.mean = np.mean(x, axis=0, keepdims=True)\n",
    "            self.var = np.var(x, axis=0, keepdims=True)\n",
    "\n",
    "            # Normalize the input\n",
    "            x_norm = (x - self.mean) / np.sqrt(self.var + 1e-8)\n",
    "\n",
    "            # Update the running statistics\n",
    "            self.running_mean = self.batch_momentum * self.running_mean + (1 - self.batch_momentum) * self.mean\n",
    "            self.running_var = self.batch_momentum * self.running_var + (1 - self.batch_momentum) * self.var\n",
    "\n",
    "        else:\n",
    "            # Use running statistics during inference\n",
    "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + 1e-8)\n",
    "\n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out, x_norm\n",
    "\n",
    "    def backward(self, grad_output, x, x_norm):\n",
    "        m = x.shape[0]  # Batch size\n",
    "\n",
    "        # Gradients with respect to gamma and beta\n",
    "        grad_gamma = np.sum(grad_output * x_norm, axis=0, keepdims=True)\n",
    "        grad_beta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients with respect to x_norm\n",
    "        grad_x_norm = grad_output * self.gamma\n",
    "\n",
    "        # Gradients with respect to variance and mean\n",
    "        grad_var = np.sum(grad_x_norm * (x - self.mean) * -0.5 * (self.var + 1e-8) ** (-3/2), axis=0, keepdims=True)\n",
    "        grad_mean = np.sum(grad_x_norm * -1 / np.sqrt(self.var + 1e-8), axis=0, keepdims=True) + \\\n",
    "                    grad_var * np.sum(-2 * (x - self.mean), axis=0, keepdims=True) / m\n",
    "\n",
    "        # Gradients with respect to x\n",
    "        grad_x = grad_x_norm / np.sqrt(self.var + 1e-8) + grad_var * 2 * (x - self.mean) / m + grad_mean / m\n",
    "\n",
    "        return grad_x, grad_gamma, grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95bff805-1c0d-4653-a4a8-ca63b75affe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompLayer():\n",
    "    def __init__(self, width, prev_layer=None, activation_function='relu', \n",
    "                 alpha=0.01, dropout_rate=0.2, batch_norm=True, \n",
    "                 batch_momentum=0.9, learning_rate=0.01):\n",
    "        self.prev_layer = prev_layer\n",
    "        self.width = width\n",
    "        self.values = None\n",
    "        self.grad = None\n",
    "        self.weights = None\n",
    "        self.grad_weights = None\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_mask = None\n",
    "        self.velocity = None\n",
    "        self.grad_squared_accum = None\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        activation_functions = {\n",
    "            'relu': relu,\n",
    "            'leaky_relu': lambda x: leaky_relu(x, alpha=alpha),\n",
    "            'sigmoid': sigmoid,\n",
    "            'tanh': tanh\n",
    "        }\n",
    "        grad_activations = {\n",
    "            'relu': relu_grad_activation,\n",
    "            'leaky_relu': lambda x: leaky_relu_grad_activation(x, alpha=alpha),\n",
    "            'sigmoid': sigmoid_grad_activation,\n",
    "            'tanh': tanh_grad_activation\n",
    "        }\n",
    "        \n",
    "        self.activation_function = activation_functions[activation_function]\n",
    "        self.grad_activation = grad_activations[activation_function]\n",
    "\n",
    "        if self.batch_norm:\n",
    "            # Instantiate BatchNormalization class\n",
    "            self.batch_norm_layer = BatchNormalization(width=self.width, batch_momentum=self.batch_momentum)\n",
    "\n",
    "    def forward(self, training=True):\n",
    "        self.inputs = np.concatenate([self.prev_layer.values, \n",
    "                                      np.ones(shape=(self.prev_layer.values.shape[0], 1))], \n",
    "                                      axis=1)\n",
    "\n",
    "        if self.weights is None:\n",
    "            np.random.seed(0)\n",
    "            if self.activation_function in [relu, leaky_relu]:\n",
    "                self.weights = he_initialize((self.width, self.inputs.shape[1]))\n",
    "            if self.activation_function in [sigmoid, tanh]:\n",
    "                self.weights = xavier_initialize((self.width, self.inputs.shape[1]))\n",
    "\n",
    "        self.z = self.inputs @ self.weights.T\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            self.z_, self.z_norm = self.batch_norm_layer.forward(self.z, training=training)\n",
    "            \n",
    "        else:\n",
    "            self.z_ = self.z\n",
    "            \n",
    "        self.values = self.activation_function(self.z_)\n",
    "\n",
    "        if training and self.dropout_rate > 0:\n",
    "            self.dropout_mask = (np.random.rand(*self.values.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            self.values *= self.dropout_mask        \n",
    "\n",
    "    def backward(self):\n",
    "        if self.dropout_rate > 0:\n",
    "            self.grad *= self.dropout_mask\n",
    "            \n",
    "        grad_activation = self.grad * self.grad_activation(self.values)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            # Backpropagate batch normalization\n",
    "            grad_z, grad_gamma, grad_beta = self.batch_norm_layer.backward(grad_activation, self.z, self.z_norm)\n",
    "            # Update scale and shift\n",
    "            self.batch_norm_layer.gamma -= self.learning_rate * grad_gamma\n",
    "            self.batch_norm_layer.beta -= self.learning_rate * grad_beta\n",
    "        else:\n",
    "            grad_z = grad_activation\n",
    "        \n",
    "        grad_input = clip_gradients(grad_activation @ self.weights[:, :-1])\n",
    "        self.prev_layer.grad = grad_input\n",
    "\n",
    "        self.grad_weights = clip_gradients(grad_activation.T @ self.inputs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "750150ae-fece-4552-8aca-71324b5e17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastLayer():\n",
    "    def __init__(self, width=None, y=None, prev_layer=None, mode='classification'):\n",
    "        self.prev_layer = prev_layer\n",
    "        self.y = y\n",
    "        np.random.seed(0)\n",
    "        self.weights = None\n",
    "        self.width=width\n",
    "        self.grad_weights = 0\n",
    "        self.velocity = None\n",
    "        self.grad_squared_accum = None\n",
    "        self.mode = mode\n",
    "\n",
    "        output_activation_functions = {\n",
    "            'regression': identity_activation,\n",
    "            'classification': softmax\n",
    "        }\n",
    "        self.evaluation = output_activation_functions[mode]        \n",
    "\n",
    "    def forward(self, training=True):\n",
    "        if self.y.ndim == 1 or self.y.shape[1] == 1:  \n",
    "            self.y = self.y.reshape(-1, 1)\n",
    "            \n",
    "        self.inputs = np.concatenate([self.prev_layer.values, \n",
    "                                      np.ones(shape=(self.prev_layer.values.shape[0], 1))], \n",
    "                                      axis=1)\n",
    "\n",
    "        if self.weights is None:\n",
    "            np.random.seed(0)\n",
    "            if self.mode == 'classification':\n",
    "                self.weights = xavier_initialize((self.width, self.inputs.shape[1]))\n",
    "            if self.mode == 'regression':\n",
    "                self.weights = he_initialize((self.width, self.inputs.shape[1]))\n",
    "            \n",
    "        self.values = self.evaluation(self.inputs @ self.weights.T)\n",
    "\n",
    "    def backward(self):        \n",
    "        self.grad = self.values - self.y\n",
    "        grad_activation = self.grad\n",
    "\n",
    "        grad_input =  clip_gradients(grad_activation @ self.weights[:, :-1])\n",
    "        self.prev_layer.grad = grad_input\n",
    "\n",
    "        self.grad_weights = clip_gradients(grad_activation.T @ self.inputs)\n",
    "\n",
    "        \n",
    "\n",
    "    def calculate_loss(self):\n",
    "        if self.mode == 'classification':\n",
    "            # Cross-entropy loss\n",
    "            return -np.sum(np.sum(self.y * np.log(self.values + 1e-8), axis=1))\n",
    "            \n",
    "        if self.mode == 'regression':\n",
    "            return np.sum((self.y - self.values) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2598399a-6651-4829-a762-eac131dc29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward(CompNodes, training=True):\n",
    "    for comp_node in CompNodes:\n",
    "        comp_node.forward(training=True)\n",
    "\n",
    "def Backward(CompNodes):\n",
    "    for comp_node in CompNodes:\n",
    "        comp_node.backward()\n",
    "\n",
    "def UpdateWeights(layers, batch_size, l1_lambda, l2_lambda, optimization, learning_rate, momentum, momentum2, iteration):\n",
    "    for layer in layers:\n",
    "        if layer.velocity is None:\n",
    "            layer.velocity = np.zeros_like(layer.weights) \n",
    "            \n",
    "        if layer.grad_squared_accum is None:\n",
    "            layer.grad_squared_accum = np.zeros_like(layer.weights)\n",
    "            \n",
    "        l1_penalty = l1_lambda * np.sign(layer.weights)\n",
    "        l2_penalty = l2_lambda * layer.weights\n",
    "\n",
    "        total_grad = (layer.grad_weights + l1_penalty + l2_penalty) / batch_size\n",
    "\n",
    "        if optimization == 'sgd':\n",
    "            layer.weights -= learning_rate * total_grad\n",
    "\n",
    "        if optimization == 'sgd_w_momentum':\n",
    "            layer.velocity = momentum * layer.velocity + total_grad  # No (1 - momentum) term\n",
    "            layer.weights -= learning_rate * layer.velocity \n",
    "\n",
    "\n",
    "        if optimization == 'RMSprop':\n",
    "            layer.grad_squared_accum = momentum2 * layer.grad_squared_accum + (1 - momentum2) * (total_grad ** 2)\n",
    "            v_hat = layer.grad_squared_accum / (1 - momentum2**iteration)\n",
    "            layer.weights -= learning_rate * (total_grad / (np.sqrt(v_hat) + 1e-8))\n",
    "\n",
    "        if optimization == 'adam':\n",
    "            layer.velocity = momentum * layer.velocity + (1 - momentum) * total_grad\n",
    "            layer.grad_squared_accum = momentum2 * layer.grad_squared_accum + (1 - momentum2) * (total_grad ** 2)\n",
    "            # bias correction\n",
    "            m_hat = layer.velocity / (1 - momentum**iteration)\n",
    "            v_hat = layer.grad_squared_accum / (1 - momentum2**iteration) \n",
    "            layer.weights -= learning_rate * (m_hat / (np.sqrt(v_hat) + 1e-8))\n",
    "\n",
    "        if optimization == 'nesterov':\n",
    "            lookahead_weights = layer.weights - learning_rate * momentum * layer.velocity\n",
    "            orig_weights = layer.weights\n",
    "            layer.weights = lookahead_weights\n",
    "            \n",
    "            Forward(layers)\n",
    "            Backward(reversed(layers[1:]))\n",
    "            \n",
    "            total_grad = (layer.grad_weights + l1_penalty + l2_penalty) / batch_size\n",
    "            \n",
    "            layer.weights = orig_weights\n",
    "            \n",
    "            layer.velocity = momentum * layer.velocity + total_grad\n",
    "            layer.weights -= learning_rate * layer.velocity \n",
    "        \n",
    "        layer.grad_weights = np.zeros_like(layer.weights)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e0ddfa3-6b01-4705-ba7d-c6dbbb61de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    def __init__(self, width_list, \n",
    "                 n_batches, n_epochs,\n",
    "                 batch_norm=True, batch_momentum=0.9, \n",
    "                 mode='classification', \n",
    "                 activation_function='relu', alpha=0.01,\n",
    "                 early_stopping=False,\n",
    "                 l1_lambda=0.0, l2_lambda=0.0,\n",
    "                 dropout_rate=0.2,\n",
    "                 optimization='sgd',\n",
    "                 learning_rate=0.01,\n",
    "                 momentum=0.9,\n",
    "                 momentum2=0.999,\n",
    "                 verbose=10):\n",
    "        \n",
    "        self.depth = len(width_list) + 1\n",
    "        self.width_list = width_list\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.n_batches = n_batches\n",
    "        self.n_epochs = n_epochs\n",
    "        self.best_n_epochs = None\n",
    "        self.train_loss = None\n",
    "        self.mode = mode\n",
    "        self.activation_function = activation_function\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.optimization = optimization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.momentum2 = momentum2\n",
    "        self.verbose = verbose\n",
    "        self.train_accuracy = []\n",
    "        self.valid_accuracy = []\n",
    "\n",
    "        self.layers = [None] * (self.depth + 1)\n",
    "        self.layers[0] = InputLayer()\n",
    "        \n",
    "        for i in range(1, self.depth):\n",
    "            self.layers[i] = CompLayer(width = self.width_list[i-1], \n",
    "                                       prev_layer=self.layers[i-1], \n",
    "                                       activation_function=self.activation_function,\n",
    "                                       dropout_rate=self.dropout_rate,\n",
    "                                       batch_norm=self.batch_norm, \n",
    "                                       batch_momentum=self.batch_momentum,\n",
    "                                       learning_rate=self.learning_rate)\n",
    "            \n",
    "        self.layers[-1] = LastLayer(prev_layer=self.layers[-2], mode=self.mode)\n",
    "        assert all(self.layers)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_valid, y_valid):\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_ = one_hot_encode_classes(y, num_classes)\n",
    "        y_valid_ = one_hot_encode_classes(y_valid, num_classes)\n",
    "        \n",
    "        self.layers[-1].width = num_classes\n",
    "                \n",
    "        batches = np.arange(0, len(X)+1, len(X) // self.n_batches)\n",
    "        batches[-1] = len(X)\n",
    "        \n",
    "        for epoch in range(1, self.n_epochs+1):\n",
    "            epoch_loss = 0\n",
    "            for i in range(1, len(batches)):\n",
    "                batch_size = batches[i] - batches[i-1]\n",
    "\n",
    "                batch_data = (X[ batches[i-1]:batches[i] ], y_[ batches[i-1]:batches[i] ])\n",
    "                batch_loss = self.train_batch(*batch_data)\n",
    "            \n",
    "                epoch_loss += batch_loss\n",
    "                UpdateWeights(layers=self.layers[1:], \n",
    "                              batch_size=batch_size,\n",
    "                              l1_lambda=self.l1_lambda, \n",
    "                              l2_lambda=self.l2_lambda,\n",
    "                              optimization=self.optimization,\n",
    "                              learning_rate=self.learning_rate,\n",
    "                              momentum=self.momentum,\n",
    "                              momentum2=self.momentum2,\n",
    "                              iteration = epoch*self.n_batches + i)\n",
    "                \n",
    "            self.train_accuracy.append(self.score(X, y))\n",
    "            self.valid_accuracy.append(self.score(X_valid, y_valid))\n",
    "\n",
    "            if ( self.verbose != -1 ) & ( epoch % self.verbose == 0 ): \n",
    "                print(f'Epoch: {epoch}, Loss: {epoch_loss / self.n_batches:.3f}', flush=True)\n",
    "                print(f'   Train Accuracy: {self.score(X, y)}', flush=True)               \n",
    "                print(f'   Valid Accuracy: {self.score(X_valid, y_valid)}\\n\\n', flush=True)\n",
    "\n",
    "    def train_batch(self, x_batch, y_batch):\n",
    "        self.layers[0].values = x_batch\n",
    "        self.layers[-1].y = y_batch\n",
    "        Forward(self.layers, training=True)\n",
    "        Backward(reversed(self.layers[1:]))\n",
    "        loss = self.layers[-1].calculate_loss()\n",
    "        l1_loss = self.l1_lambda * sum(np.sum(np.abs(layer.weights))for layer in self.layers[1:])\n",
    "        l2_loss = self.l2_lambda * sum(np.sum(layer.weights**2)for layer in self.layers[1:])\n",
    "\n",
    "        return (loss + l1_loss + l2_loss) / len(x_batch)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.layers[0].values = X\n",
    "        Forward(self.layers, training=False)\n",
    "        preds = self.layers[-1].values\n",
    "        \n",
    "        return np.argmax(preds, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        if self.mode == 'regression':\n",
    "            return round(mean_absolute_error(preds, y), 3)\n",
    "        \n",
    "        if self.mode == 'classification':\n",
    "            return round(accuracy(preds, y), 3)\n",
    "\n",
    "    def get_train_accuracy(self):\n",
    "        return self.train_accuracy\n",
    "\n",
    "    def get_valid_accuracy(self):\n",
    "        return self.valid_accuracy\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5485e04-8797-4a26-8929-7be7acceacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (48000, 784)\n",
      "y_train shape: (48000, 784)\n",
      "x_valid shape: (12000, 784)\n",
      "y_valid shape: (12000, 784)\n",
      "x_test shape: (10000, 784)\n",
      "y_test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "mnist = datasets.fetch_openml('mnist_784', version=1)\n",
    "\n",
    "x_train = mnist.data[:60000].to_numpy()\n",
    "y_train = mnist.target[:60000].astype(int).to_numpy()\n",
    "x_test = mnist.data[60000:].to_numpy()\n",
    "y_test = mnist.target[60000:].astype(int).to_numpy()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_valid = x_valid / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {x_train.shape}')\n",
    "\n",
    "print(f'x_valid shape: {x_valid.shape}')\n",
    "print(f'y_valid shape: {x_valid.shape}')\n",
    "\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "print(f'y_test shape: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bddde5e6-ca9f-4c4c-a0e0-60ea9db545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(\n",
    "    width_list=[256],\n",
    "    n_batches=16, \n",
    "    n_epochs=100, \n",
    "    batch_norm=True,\n",
    "    batch_momentum=0.9,\n",
    "    mode='classification', \n",
    "    activation_function='relu',\n",
    "    alpha=1,\n",
    "    l1_lambda=0.0001,\n",
    "    l2_lambda=0.001,\n",
    "    dropout_rate=0.2,\n",
    "    optimization='adam',\n",
    "    learning_rate=0.001,\n",
    "    momentum=0.9,\n",
    "    momentum2=0.999,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "397d0c0a-118c-4184-a4ce-de2c29ec7286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.189\n",
      "   Train Accuracy: 0.948\n",
      "   Valid Accuracy: 0.942\n",
      "\n",
      "\n",
      "Epoch: 20, Loss: 0.122\n",
      "   Train Accuracy: 0.967\n",
      "   Valid Accuracy: 0.956\n",
      "\n",
      "\n",
      "Epoch: 30, Loss: 0.091\n",
      "   Train Accuracy: 0.974\n",
      "   Valid Accuracy: 0.963\n",
      "\n",
      "\n",
      "Epoch: 40, Loss: 0.074\n",
      "   Train Accuracy: 0.979\n",
      "   Valid Accuracy: 0.965\n",
      "\n",
      "\n",
      "Epoch: 50, Loss: 0.059\n",
      "   Train Accuracy: 0.983\n",
      "   Valid Accuracy: 0.967\n",
      "\n",
      "\n",
      "Epoch: 60, Loss: 0.049\n",
      "   Train Accuracy: 0.987\n",
      "   Valid Accuracy: 0.969\n",
      "\n",
      "\n",
      "Epoch: 70, Loss: 0.042\n",
      "   Train Accuracy: 0.989\n",
      "   Valid Accuracy: 0.97\n",
      "\n",
      "\n",
      "Epoch: 80, Loss: 0.036\n",
      "   Train Accuracy: 0.99\n",
      "   Valid Accuracy: 0.97\n",
      "\n",
      "\n",
      "Epoch: 90, Loss: 0.032\n",
      "   Train Accuracy: 0.992\n",
      "   Valid Accuracy: 0.969\n",
      "\n",
      "\n",
      "Epoch: 100, Loss: 0.028\n",
      "   Train Accuracy: 0.993\n",
      "   Valid Accuracy: 0.972\n",
      "\n",
      "\n",
      "Final Train accuracy: 0.993\n",
      "Final Valid accuracy: 0.972\n",
      "Final Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "\n",
    "print(f'Final Train accuracy: {mlp.score(x_train, y_train)}')\n",
    "print(f'Final Valid accuracy: {mlp.score(x_valid, y_valid)}')\n",
    "print(f'Final Test accuracy: {mlp.score(x_test, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
